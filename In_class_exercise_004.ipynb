{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ShreshtaP/-Shreshta_INFO5731_-Fall2021/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwhqVpSWfGbq"
   },
   "source": [
    "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMiwF_OsfGbt"
   },
   "source": [
    "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUgpX5RIfGbv"
   },
   "source": [
    "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GklcnUkkyiX",
    "outputId": "0c22e77f-d482-46ea-eeb3-10531ba7b7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import lxml.html as LH\n",
    "url = 'https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3'\n",
    "imdb_url = \"https://www.imdb.com\"\n",
    "response = requests.get(url)\n",
    "movies_soup = BeautifulSoup(response.text, 'lxml')\n",
    "movie_tags = movies_soup.find_all('a', attrs={'class': None})\n",
    "movie_tags = [tag.attrs['href'] for tag in movie_tags \n",
    "              if tag.attrs['href'].startswith('/title') & tag.attrs['href'].endswith('/')]\n",
    "movie_tags = list(dict.fromkeys(movie_tags))\n",
    "movie_links = [imdb_url + tag + 'reviews' for tag in movie_tags]\n",
    "def minMax(a):\n",
    "      print(len(a))\n",
    "      minpos = a.index(min(a))\n",
    "      maxpos = a.index(max(a))\n",
    "      return minpos, maxpos\n",
    "def getReviews(soup):\n",
    "    review_ratings = [tag.previous_element for tag in \n",
    "                           soup.find_all('span', attrs={'class': 'point-scale'})]\n",
    "    if len(review_ratings) > 0:\n",
    "      n_index, p_index = minMax(list(map(int, review_ratings)))\n",
    "      review_list = soup.find_all('a', attrs={'class':'title'})\n",
    "      reviews = [\"https://www.imdb.com\" + review['href'] for review in review_list]\n",
    "      return reviews\n",
    "    else:\n",
    "      return None\n",
    "def getSoup(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "def getReviewText(review_url):\n",
    "    soup = getSoup(review_url)\n",
    "    tag = soup.find('div', attrs={'class': 'text show-more__control'})\n",
    "    return tag.getText()\n",
    "movie_soups = getSoup(movie_links[0])\n",
    "movie_review = getReviews(movie_soups)\n",
    "imdb_review_texts = [getReviewText(url) for url in movie_review]\n",
    "user_Names = [getUserName(url) for url in movie_review]\n",
    "\n",
    "posted_dates = [getReviewPostedDate(url) for url in movie_review]\n",
    "data = {'Review_text': imdb_review_texts,\n",
    "        }\n",
    "df = pd.DataFrame(data) \n",
    "df.to_csv(\"IMDB_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_cIzKdQ2lVv4",
    "outputId": "d2329765-79f9-4520-b40a-7c5f829a6a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5hfPx2WCljJn",
    "outputId": "8cfd630e-d790-4199-afec-644249fe2a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBFNEcFolnNu",
    "outputId": "79f8c8e5-07d9-4534-8662-23840e85d944"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "openjdk version \"11.0.11\" 2021-04-20\n",
      "OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "def install_java():\n",
    "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "  !java -version\n",
    "install_java()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwfOAy-HlvZo",
    "outputId": "18285b6c-50f9-49d6-bd22-2fb0918469c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.22.2.post1)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "fXWDH1fQm4PS"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "EXMtcIQem8Go"
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "rLfuLayVm_bX"
   },
   "outputs": [],
   "source": [
    "# # Plotting tools\n",
    "# import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4y-ZWfTYnM09"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('englishlanguage')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5K5JF-KnRT4",
    "outputId": "6a72b71e-c774-41f5-f374-e0eee0538c3a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB_reviews.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "# Convert to list\n",
    "dataretrieved = df['Review_text'].tolist()\n",
    "\n",
    "# Remove Emails\n",
    "dataretrieved = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "dataretrieved = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "dataretrieved = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(dataretrieved[:1])\n",
    "\n",
    "#Tokenization\n",
    "def sent_to_words(sentencestatement):\n",
    "    for sentence in sentencestatement:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "no_of_data_words = list(sent_to_words(dataretrieved))\n",
    "\n",
    "print(no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5K5JF-KnRT4",
    "outputId": "6a72b71e-c774-41f5-f374-e0eee0538c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was an okay movie. Simu Liu is the son of the seemingly immortal master '\n",
      " 'of the Ten Rings --always good to see Tony Leung, and I hope he soaked '\n",
      " 'Disney --who winds up opposing his dad, and Awkwafina is his slacker sort-of '\n",
      " 'girlfriend. They are fine in the leads, as is Menger Zhang as his sister, '\n",
      " 'and Michelle Yeoh as their aunt -- always good to see her, and I hope she '\n",
      " 'soaked Disney too.I have grown increasingly fond of martial arts films over '\n",
      " 'the last couple of decades, and the decline of the movie musical is the '\n",
      " 'reason why. They dont make movies like that any more, and when they try, '\n",
      " 'theyre often embarrassing. People dont like the unreality of people bursting '\n",
      " 'out into song, or expressing their emotions through dance. No, they prefer '\n",
      " 'the solid commonplace of people floating through space to kick each other in '\n",
      " 'the head. To me, the fight choreography is its own dance, and the fight '\n",
      " 'between Leung and Fala Chen when they meet and before they fall in love is a '\n",
      " 'challenge dance as graceful as any between Astaire and Rogers.There are '\n",
      " 'problems aplenty with this film. Theres an immense amount of exposition, '\n",
      " 'starting off with twenty minutes of it in Chinese. Im not sure why Ben '\n",
      " 'Kingsley as the fake Mandarin from IRON MAN 3 is present except for a '\n",
      " 'certain remnant of star power and connecting this to the rest of the MCU. '\n",
      " 'Still, with my expectations not set particularly high, I had a good time.']\n",
      "[['it', 'was', 'an', 'okay', 'movie', 'simu', 'liu', 'is', 'the', 'son', 'of', 'the', 'seemingly', 'immortal', 'master', 'of', 'the', 'ten', 'rings', 'always', 'good', 'to', 'see', 'tony', 'leung', 'and', 'hope', 'he', 'soaked', 'disney', 'who', 'winds', 'up', 'opposing', 'his', 'dad', 'and', 'awkwafina', 'is', 'his', 'slacker', 'sort', 'of', 'girlfriend', 'they', 'are', 'fine', 'in', 'the', 'leads', 'as', 'is', 'menger', 'zhang', 'as', 'his', 'sister', 'and', 'michelle', 'yeoh', 'as', 'their', 'aunt', 'always', 'good', 'to', 'see', 'her', 'and', 'hope', 'she', 'soaked', 'disney', 'too', 'have', 'grown', 'increasingly', 'fond', 'of', 'martial', 'arts', 'films', 'over', 'the', 'last', 'couple', 'of', 'decades', 'and', 'the', 'decline', 'of', 'the', 'movie', 'musical', 'is', 'the', 'reason', 'why', 'they', 'dont', 'make', 'movies', 'like', 'that', 'any', 'more', 'and', 'when', 'they', 'try', 'theyre', 'often', 'embarrassing', 'people', 'dont', 'like', 'the', 'unreality', 'of', 'people', 'bursting', 'out', 'into', 'song', 'or', 'expressing', 'their', 'emotions', 'through', 'dance', 'no', 'they', 'prefer', 'the', 'solid', 'commonplace', 'of', 'people', 'floating', 'through', 'space', 'to', 'kick', 'each', 'other', 'in', 'the', 'head', 'to', 'me', 'the', 'fight', 'choreography', 'is', 'its', 'own', 'dance', 'and', 'the', 'fight', 'between', 'leung', 'and', 'fala', 'chen', 'when', 'they', 'meet', 'and', 'before', 'they', 'fall', 'in', 'love', 'is', 'challenge', 'dance', 'as', 'graceful', 'as', 'any', 'between', 'astaire', 'and', 'rogers', 'there', 'are', 'problems', 'aplenty', 'with', 'this', 'film', 'theres', 'an', 'immense', 'amount', 'of', 'exposition', 'starting', 'off', 'with', 'twenty', 'minutes', 'of', 'it', 'in', 'chinese', 'im', 'not', 'sure', 'why', 'ben', 'kingsley', 'as', 'the', 'fake', 'mandarin', 'from', 'iron', 'man', 'is', 'present', 'except', 'for', 'certain', 'remnant', 'of', 'star', 'power', 'and', 'connecting', 'this', 'to', 'the', 'rest', 'of', 'the', 'mcu', 'still', 'with', 'my', 'expectations', 'not', 'set', 'particularly', 'high', 'had', 'good', 'time']]\n",
      "['it', 'was', 'an', 'okay', 'movie', 'simu_liu', 'is', 'the', 'son', 'of', 'the', 'seemingly', 'immortal', 'master', 'of', 'the', 'ten_rings', 'always', 'good', 'to', 'see', 'tony_leung', 'and', 'hope', 'he', 'soaked', 'disney', 'who', 'winds', 'up', 'opposing', 'his', 'dad', 'and', 'awkwafina', 'is', 'his', 'slacker', 'sort', 'of', 'girlfriend', 'they', 'are', 'fine', 'in', 'the', 'leads', 'as', 'is', 'menger_zhang', 'as', 'his', 'sister', 'and', 'michelle_yeoh', 'as', 'their', 'aunt', 'always', 'good', 'to', 'see', 'her', 'and', 'hope', 'she', 'soaked', 'disney', 'too', 'have', 'grown', 'increasingly', 'fond', 'of', 'martial_arts', 'films', 'over', 'the', 'last', 'couple', 'of', 'decades', 'and', 'the', 'decline', 'of', 'the', 'movie', 'musical', 'is', 'the', 'reason', 'why', 'they', 'dont', 'make', 'movies', 'like', 'that', 'any', 'more', 'and', 'when', 'they', 'try', 'theyre', 'often', 'embarrassing', 'people', 'dont', 'like', 'the', 'unreality', 'of', 'people', 'bursting', 'out', 'into', 'song', 'or', 'expressing', 'their', 'emotions', 'through', 'dance', 'no', 'they', 'prefer', 'the', 'solid', 'commonplace', 'of', 'people', 'floating', 'through', 'space', 'to', 'kick', 'each', 'other', 'in', 'the', 'head', 'to', 'me', 'the', 'fight', 'choreography', 'is', 'its', 'own', 'dance', 'and', 'the', 'fight', 'between', 'leung', 'and', 'fala', 'chen', 'when', 'they', 'meet', 'and', 'before', 'they', 'fall', 'in', 'love', 'is', 'challenge', 'dance', 'as', 'graceful', 'as', 'any', 'between', 'astaire', 'and', 'rogers', 'there', 'are', 'problems', 'aplenty', 'with', 'this', 'film', 'theres', 'an', 'immense', 'amount', 'of', 'exposition', 'starting', 'off', 'with', 'twenty', 'minutes', 'of', 'it', 'in', 'chinese', 'im', 'not', 'sure', 'why', 'ben', 'kingsley', 'as', 'the', 'fake', 'mandarin', 'from', 'iron', 'man', 'is', 'present', 'except', 'for', 'certain', 'remnant', 'of', 'star', 'power', 'and', 'connecting', 'this', 'to', 'the', 'rest', 'of', 'the', 'mcu', 'still', 'with', 'my', 'expectations', 'not', 'set', 'particularly', 'high', 'had', 'good', 'time']\n",
      "[['son', 'seemingly', 'immortal', 'master', 'always', 'good', 'see', 'hope', 'soak', 'wind', 'oppose', 'girlfriend', 'fine', 'lead', 'sister', 'aunt', 'always', 'good', 'see', 'hope', 'soak', 'grow', 'increasingly', 'fond', 'film', 'last', 'couple', 'decade', 'decline', 'movie', 'musical', 'reason', 'make', 'movie', 'try', 'be', 'often', 'embarrassing', 'people', 'like', 'unreality', 'people', 'burst', 'song', 'express', 'emotion', 'dance', 'prefer', 'solid', 'commonplace', 'people', 'float', 'space', 'kick', 'head', 'fight', 'choreography', 'dance', 'fight', 'love', 'challenge', 'dance', 'graceful', 'astaire', 'roger', 'problem', 'aplenty', 'film', 's', 'immense', 'amount', 'exposition', 'start', 'minute', 'chinese', 'be', 'sure', 'iron', 'man', 'present', 'certain', 'remnant', 'star', 'power', 'connect', 'rest', 'still', 'expectation', 'set', 'particularly', 'high', 'good', 'time']]\n",
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 3), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 2), (24, 1), (25, 1), (26, 1), (27, 1), (28, 3), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 2), (48, 1), (49, 1), (50, 1), (51, 1), (52, 3), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 2), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('always', 2),\n",
       "  ('amount', 1),\n",
       "  ('aplenty', 1),\n",
       "  ('astaire', 1),\n",
       "  ('aunt', 1),\n",
       "  ('be', 2),\n",
       "  ('burst', 1),\n",
       "  ('certain', 1),\n",
       "  ('challenge', 1),\n",
       "  ('chinese', 1),\n",
       "  ('choreography', 1),\n",
       "  ('commonplace', 1),\n",
       "  ('connect', 1),\n",
       "  ('couple', 1),\n",
       "  ('dance', 3),\n",
       "  ('decade', 1),\n",
       "  ('decline', 1),\n",
       "  ('embarrassing', 1),\n",
       "  ('emotion', 1),\n",
       "  ('expectation', 1),\n",
       "  ('exposition', 1),\n",
       "  ('express', 1),\n",
       "  ('fight', 2),\n",
       "  ('film', 2),\n",
       "  ('fine', 1),\n",
       "  ('float', 1),\n",
       "  ('fond', 1),\n",
       "  ('girlfriend', 1),\n",
       "  ('good', 3),\n",
       "  ('graceful', 1),\n",
       "  ('grow', 1),\n",
       "  ('head', 1),\n",
       "  ('high', 1),\n",
       "  ('hope', 2),\n",
       "  ('immense', 1),\n",
       "  ('immortal', 1),\n",
       "  ('increasingly', 1),\n",
       "  ('iron', 1),\n",
       "  ('kick', 1),\n",
       "  ('last', 1),\n",
       "  ('lead', 1),\n",
       "  ('like', 1),\n",
       "  ('love', 1),\n",
       "  ('make', 1),\n",
       "  ('man', 1),\n",
       "  ('master', 1),\n",
       "  ('minute', 1),\n",
       "  ('movie', 2),\n",
       "  ('musical', 1),\n",
       "  ('often', 1),\n",
       "  ('oppose', 1),\n",
       "  ('particularly', 1),\n",
       "  ('people', 3),\n",
       "  ('power', 1),\n",
       "  ('prefer', 1),\n",
       "  ('present', 1),\n",
       "  ('problem', 1),\n",
       "  ('reason', 1),\n",
       "  ('remnant', 1),\n",
       "  ('rest', 1),\n",
       "  ('roger', 1),\n",
       "  ('s', 1),\n",
       "  ('see', 2),\n",
       "  ('seemingly', 1),\n",
       "  ('set', 1),\n",
       "  ('sister', 1),\n",
       "  ('soak', 2),\n",
       "  ('solid', 1),\n",
       "  ('son', 1),\n",
       "  ('song', 1),\n",
       "  ('space', 1),\n",
       "  ('star', 1),\n",
       "  ('start', 1),\n",
       "  ('still', 1),\n",
       "  ('sure', 1),\n",
       "  ('time', 1),\n",
       "  ('try', 1),\n",
       "  ('unreality', 1),\n",
       "  ('wind', 1)]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB_reviews.csv', encoding = \"ISO-8859-1\")\n",
    "df.head()\n",
    "\n",
    "# Convert to list\n",
    "dataretrieved = df['Review_text'].tolist()\n",
    "\n",
    "# Remove Emails\n",
    "dataretrieved = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "dataretrieved = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "dataretrieved = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(dataretrieved[:1])\n",
    "\n",
    "#Tokenization\n",
    "def sent_to_words(sentencestatement):\n",
    "    for sentence in sentencestatement:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "no_of_data_words = list(sent_to_words(dataretrieved))\n",
    "\n",
    "print(no_of_data_words[:1])\n",
    "\n",
    "# Creating Bigram and Trigram Models\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "\n",
    "\n",
    "\n",
    "#Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "\n",
    "#Create the Dictionary and Corpus needed for Topic Modeling\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uSV9Khg_n18c",
    "outputId": "e4e72505-f259-4c6c-b3be-0dac540d7317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.015*\"character\" + 0.012*\"film\" + 0.012*\"new\" + 0.010*\"fight\" + '\n",
      "  '0.010*\"marvel\" + 0.009*\"year\" + 0.007*\"bus\" + 0.007*\"first\" + 0.007*\"end\" + '\n",
      "  '0.007*\"action\"'),\n",
      " (1,\n",
      "  '0.001*\"movie\" + 0.001*\"good\" + 0.001*\"year\" + 0.001*\"go\" + 0.001*\"have\" + '\n",
      "  '0.001*\"film\" + 0.001*\"character\" + 0.001*\"love\" + 0.001*\"would\" + '\n",
      "  '0.001*\"wenwu\"'),\n",
      " (2,\n",
      "  '0.008*\"scene\" + 0.008*\"feature\" + 0.008*\"rescue\" + 0.008*\"planet\" + '\n",
      "  '0.004*\"good\" + 0.004*\"see\" + 0.004*\"story\" + 0.004*\"also\" + 0.004*\"watch\" + '\n",
      "  '0.004*\"great\"'),\n",
      " (3,\n",
      "  '0.019*\"show\" + 0.011*\"lead\" + 0.010*\"talent\" + 0.010*\"spoiler\" + '\n",
      "  '0.010*\"suddenly\" + 0.009*\"word\" + 0.007*\"like\" + 0.007*\"definitely\" + '\n",
      "  '0.006*\"actor\" + 0.005*\"panther\"'),\n",
      " (4,\n",
      "  '0.032*\"great\" + 0.022*\"film\" + 0.015*\"action\" + 0.011*\"good\" + '\n",
      "  '0.011*\"character\" + 0.011*\"story\" + 0.011*\"exciting\" + 0.011*\"set\" + '\n",
      "  '0.007*\"well\" + 0.007*\"say\"'),\n",
      " (5,\n",
      "  '0.025*\"movie\" + 0.011*\"character\" + 0.011*\"good\" + 0.010*\"would\" + '\n",
      "  '0.010*\"scene\" + 0.009*\"film\" + 0.009*\"have\" + 0.007*\"give\" + 0.007*\"love\" + '\n",
      "  '0.007*\"make\"'),\n",
      " (6,\n",
      "  '0.013*\"dweller\" + 0.013*\"wenwu\" + 0.010*\"attack\" + 0.010*\"go\" + '\n",
      "  '0.008*\"escape\" + 0.008*\"darkness\" + 0.008*\"great\" + 0.008*\"iron\" + '\n",
      "  '0.008*\"fight\" + 0.008*\"good\"'),\n",
      " (7,\n",
      "  '0.023*\"film\" + 0.013*\"see\" + 0.012*\"good\" + 0.010*\"people\" + 0.010*\"love\" + '\n",
      "  '0.010*\"dance\" + 0.007*\"year\" + 0.007*\"movie\" + 0.007*\"go\" + 0.007*\"fight\"'),\n",
      " (8,\n",
      "  '0.008*\"movie\" + 0.008*\"wenwu\" + 0.008*\"monster\" + 0.008*\"go\" + 0.008*\"may\" '\n",
      "  '+ 0.008*\"change\" + 0.008*\"way\" + 0.008*\"easy\" + 0.008*\"believe\" + '\n",
      "  '0.008*\"call\"')]\n",
      "\n",
      "Perplexity:  -7.243835328445328\n",
      "\n",
      "Coherence Score:  0.3757168502177676\n"
     ]
    }
   ],
   "source": [
    "# Building the Topic Model\n",
    "\n",
    "# Build LDA model\n",
    "lda_model_sample_ = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=9, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "#View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model_sample.print_topics())\n",
    "doc_lda_sample = lda_model_sample[corpus]\n",
    "\n",
    "#Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PoVUodhohCb",
    "outputId": "7735f45c-49d5-421f-e78b-3461a8495362"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.017*\"character\" + 0.011*\"family\" + 0.011*\"fight\" + 0.011*\"bus\" + '\n",
      "  '0.011*\"sequence\" + 0.011*\"entertaining\" + 0.011*\"almost\" + '\n",
      "  '0.011*\"cinematic\" + 0.011*\"serious\" + 0.006*\"year\"'),\n",
      " (1,\n",
      "  '0.019*\"good\" + 0.019*\"film\" + 0.019*\"really\" + 0.010*\"well\" + '\n",
      "  '0.010*\"character\" + 0.010*\"action\" + 0.010*\"story\" + 0.010*\"marvel\" + '\n",
      "  '0.010*\"effect\" + 0.010*\"may\"'),\n",
      " (2,\n",
      "  '0.001*\"love\" + 0.001*\"would\" + 0.001*\"movie\" + 0.001*\"scene\" + 0.001*\"good\" '\n",
      "  '+ 0.001*\"make\" + 0.001*\"have\" + 0.001*\"also\" + 0.001*\"character\" + '\n",
      "  '0.001*\"bit\"'),\n",
      " (3,\n",
      "  '0.001*\"scene\" + 0.001*\"good\" + 0.001*\"movie\" + 0.001*\"fight\" + '\n",
      "  '0.001*\"rescue\" + 0.001*\"see\" + 0.001*\"feature\" + 0.001*\"planet\" + '\n",
      "  '0.001*\"great\" + 0.001*\"lead\"'),\n",
      " (4,\n",
      "  '0.022*\"also\" + 0.013*\"movie\" + 0.013*\"scene\" + 0.013*\"good\" + '\n",
      "  '0.013*\"especially\" + 0.013*\"make\" + 0.013*\"credit\" + 0.013*\"big\" + '\n",
      "  '0.013*\"quite\" + 0.009*\"thing\"'),\n",
      " (5,\n",
      "  '0.022*\"movie\" + 0.013*\"have\" + 0.012*\"good\" + 0.012*\"would\" + 0.011*\"scene\" '\n",
      "  '+ 0.010*\"fight\" + 0.010*\"year\" + 0.009*\"love\" + 0.008*\"film\" + '\n",
      "  '0.008*\"character\"'),\n",
      " (6,\n",
      "  '0.016*\"defeat\" + 0.013*\"saun\" + 0.010*\"die\" + 0.010*\"sound\" + '\n",
      "  '0.010*\"breach\" + 0.010*\"try\" + 0.010*\"butt\" + 0.010*\"cause\" + 0.010*\"brick\" '\n",
      "  '+ 0.009*\"success\"'),\n",
      " (7,\n",
      "  '0.001*\"movie\" + 0.001*\"year\" + 0.001*\"good\" + 0.001*\"lot\" + 0.001*\"go\" + '\n",
      "  '0.001*\"people\" + 0.001*\"fight\" + 0.001*\"film\" + 0.001*\"time\" + 0.001*\"s\"'),\n",
      " (8,\n",
      "  '0.039*\"movie\" + 0.033*\"play\" + 0.013*\"marvel\" + 0.013*\"watch\" + '\n",
      "  '0.013*\"action\" + 0.013*\"future\" + 0.013*\"amazing\" + 0.007*\"also\" + '\n",
      "  '0.007*\"many\" + 0.007*\"comic\"'),\n",
      " (9,\n",
      "  '0.020*\"film\" + 0.017*\"great\" + 0.010*\"good\" + 0.010*\"fight\" + '\n",
      "  '0.008*\"people\" + 0.008*\"character\" + 0.008*\"story\" + 0.008*\"make\" + '\n",
      "  '0.008*\"new\" + 0.007*\"movie\"'),\n",
      " (10,\n",
      "  '0.022*\"movie\" + 0.015*\"character\" + 0.013*\"show\" + 0.010*\"play\" + '\n",
      "  '0.010*\"act\" + 0.010*\"get\" + 0.010*\"know\" + 0.010*\"good\" + 0.010*\"would\" + '\n",
      "  '0.008*\"expect\"'),\n",
      " (11,\n",
      "  '0.001*\"go\" + 0.001*\"film\" + 0.001*\"look\" + 0.001*\"even\" + 0.001*\"new\" + '\n",
      "  '0.001*\"superhero\" + 0.001*\"dweller\" + 0.001*\"make\" + 0.001*\"fight\" + '\n",
      "  '0.001*\"good\"'),\n",
      " (12,\n",
      "  '0.020*\"film\" + 0.016*\"first\" + 0.012*\"marvel\" + 0.012*\"action\" + 0.012*\"s\" '\n",
      "  '+ 0.012*\"year\" + 0.008*\"good\" + 0.008*\"visual\" + 0.008*\"rush\" + '\n",
      "  '0.008*\"type\"'),\n",
      " (13,\n",
      "  '0.012*\"see\" + 0.012*\"time\" + 0.009*\"go\" + 0.009*\"even\" + 0.009*\"film\" + '\n",
      "  '0.009*\"lot\" + 0.009*\"belong\" + 0.009*\"feel\" + 0.009*\"people\" + '\n",
      "  '0.006*\"movie\"'),\n",
      " (14,\n",
      "  '0.011*\"movie\" + 0.011*\"way\" + 0.011*\"believe\" + 0.011*\"go\" + 0.011*\"live\" + '\n",
      "  '0.011*\"may\" + 0.011*\"danger\" + 0.011*\"superhero\" + 0.011*\"friend\" + '\n",
      "  '0.011*\"call\"'),\n",
      " (15,\n",
      "  '0.019*\"dweller\" + 0.017*\"wenwu\" + 0.014*\"attack\" + 0.012*\"action\" + '\n",
      "  '0.012*\"save\" + 0.012*\"escape\" + 0.012*\"lead\" + 0.012*\"film\" + '\n",
      "  '0.012*\"darkness\" + 0.011*\"year\"')]\n",
      "\n",
      "Perplexity:  -7.455226788964729\n",
      "\n",
      "Coherence Score:  0.3287424635058203\n"
     ]
    }
   ],
   "source": [
    "# Building the Topic Model with number of topics as \n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=16, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "#View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "#Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0zHlN_3Wojoz",
    "outputId": "a9c16e0b-66a4-46e1-bc3d-9c8b22d85877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"character\" + 0.010*\"bus\" + 0.010*\"fight\" + 0.010*\"family\" + '\n",
      "  '0.010*\"entertaining\" + 0.010*\"sequence\" + 0.010*\"almost\" + '\n",
      "  '0.010*\"cinematic\" + 0.010*\"serious\" + 0.005*\"year\"'),\n",
      " (1,\n",
      "  '0.001*\"movie\" + 0.001*\"have\" + 0.001*\"year\" + 0.001*\"would\" + 0.001*\"good\" '\n",
      "  '+ 0.001*\"come\" + 0.001*\"love\" + 0.001*\"character\" + 0.001*\"give\" + '\n",
      "  '0.001*\"go\"'),\n",
      " (2,\n",
      "  '0.010*\"scene\" + 0.010*\"feature\" + 0.010*\"planet\" + 0.010*\"rescue\" + '\n",
      "  '0.005*\"good\" + 0.005*\"also\" + 0.005*\"watch\" + 0.005*\"well\" + 0.005*\"quite\" '\n",
      "  '+ 0.005*\"see\"'),\n",
      " (3,\n",
      "  '0.001*\"movie\" + 0.001*\"love\" + 0.001*\"good\" + 0.001*\"scene\" + 0.001*\"see\" + '\n",
      "  '0.001*\"fight\" + 0.001*\"story\" + 0.001*\"go\" + 0.001*\"know\" + '\n",
      "  '0.001*\"character\"'),\n",
      " (4,\n",
      "  '0.020*\"also\" + 0.012*\"movie\" + 0.012*\"good\" + 0.012*\"scene\" + '\n",
      "  '0.012*\"especially\" + 0.012*\"make\" + 0.012*\"big\" + 0.012*\"quite\" + '\n",
      "  '0.012*\"credit\" + 0.008*\"thing\"'),\n",
      " (5,\n",
      "  '0.021*\"movie\" + 0.011*\"good\" + 0.010*\"scene\" + 0.009*\"would\" + 0.009*\"love\" '\n",
      "  '+ 0.009*\"film\" + 0.008*\"character\" + 0.008*\"fight\" + 0.007*\"give\" + '\n",
      "  '0.007*\"have\"'),\n",
      " (6,\n",
      "  '0.029*\"year\" + 0.020*\"saun\" + 0.020*\"would\" + 0.018*\"have\" + 0.015*\"find\" + '\n",
      "  '0.014*\"old\" + 0.014*\"life\" + 0.013*\"power\" + 0.013*\"could\" + '\n",
      "  '0.012*\"defeat\"'),\n",
      " (7,\n",
      "  '0.012*\"movie\" + 0.012*\"film\" + 0.012*\"lot\" + 0.012*\"people\" + '\n",
      "  '0.012*\"chinese\" + 0.012*\"s\" + 0.012*\"amazing\" + 0.006*\"go\" + 0.006*\"fight\" '\n",
      "  '+ 0.006*\"good\"'),\n",
      " (8,\n",
      "  '0.035*\"movie\" + 0.029*\"play\" + 0.012*\"watch\" + 0.012*\"marvel\" + '\n",
      "  '0.012*\"action\" + 0.012*\"amazing\" + 0.012*\"future\" + 0.006*\"also\" + '\n",
      "  '0.006*\"give\" + 0.006*\"many\"'),\n",
      " (9,\n",
      "  '0.022*\"film\" + 0.020*\"great\" + 0.012*\"new\" + 0.011*\"character\" + '\n",
      "  '0.009*\"make\" + 0.009*\"set\" + 0.008*\"go\" + 0.008*\"look\" + 0.008*\"superhero\" '\n",
      "  '+ 0.008*\"fight\"'),\n",
      " (10,\n",
      "  '0.016*\"time\" + 0.015*\"character\" + 0.012*\"film\" + 0.012*\"good\" + '\n",
      "  '0.009*\"lot\" + 0.008*\"well\" + 0.008*\"expect\" + 0.008*\"see\" + 0.008*\"belong\" '\n",
      "  '+ 0.008*\"family\"'),\n",
      " (11,\n",
      "  '0.001*\"year\" + 0.001*\"film\" + 0.001*\"go\" + 0.001*\"good\" + '\n",
      "  '0.001*\"absolutely\" + 0.001*\"take\" + 0.001*\"see\" + 0.001*\"dweller\" + '\n",
      "  '0.001*\"would\" + 0.001*\"movie\"'),\n",
      " (12,\n",
      "  '0.013*\"film\" + 0.011*\"type\" + 0.011*\"action\" + 0.011*\"first\" + 0.008*\"good\" '\n",
      "  '+ 0.008*\"movie\" + 0.008*\"fight\" + 0.008*\"may\" + 0.008*\"live\" + '\n",
      "  '0.008*\"marvel\"')]\n",
      "\n",
      "Perplexity:  -7.386485341665187\n",
      "\n",
      "Coherence Score:  0.3231762006260269\n"
     ]
    }
   ],
   "source": [
    "# Building the Topic Model with number of topics as \n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=13, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "#View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "#Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better. \n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v') \n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzhZM5jkvWcv",
    "outputId": "94d85775-c5ae-440d-aff8-ce63b6e2d29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.018*\"character\" + 0.012*\"entertaining\" + 0.012*\"bus\" + 0.012*\"sequence\" + '\n",
      "  '0.012*\"almost\" + 0.012*\"serious\" + 0.012*\"family\" + 0.012*\"cinematic\" + '\n",
      "  '0.012*\"fight\" + 0.006*\"year\"'),\n",
      " (1,\n",
      "  '0.041*\"movie\" + 0.034*\"play\" + 0.014*\"action\" + 0.014*\"marvel\" + '\n",
      "  '0.014*\"watch\" + 0.014*\"future\" + 0.014*\"amazing\" + 0.007*\"give\" + '\n",
      "  '0.007*\"xiale\" + 0.007*\"many\"'),\n",
      " (2,\n",
      "  '0.009*\"love\" + 0.009*\"do\" + 0.009*\"story\" + 0.009*\"see\" + 0.009*\"must\" + '\n",
      "  '0.009*\"rescue\" + 0.009*\"scene\" + 0.009*\"planet\" + 0.009*\"feature\" + '\n",
      "  '0.005*\"make\"'),\n",
      " (3,\n",
      "  '0.001*\"movie\" + 0.001*\"good\" + 0.001*\"fight\" + 0.001*\"scene\" + 0.001*\"lead\" '\n",
      "  '+ 0.001*\"know\" + 0.001*\"attack\" + 0.001*\"show\" + 0.001*\"great\" + '\n",
      "  '0.001*\"go\"'),\n",
      " (4,\n",
      "  '0.001*\"movie\" + 0.001*\"good\" + 0.001*\"scene\" + 0.001*\"also\" + 0.001*\"time\" '\n",
      "  '+ 0.001*\"thing\" + 0.001*\"especially\" + 0.001*\"watch\" + 0.001*\"lot\" + '\n",
      "  '0.001*\"character\"'),\n",
      " (5,\n",
      "  '0.024*\"movie\" + 0.012*\"would\" + 0.012*\"good\" + 0.011*\"have\" + '\n",
      "  '0.010*\"character\" + 0.010*\"love\" + 0.010*\"scene\" + 0.008*\"year\" + '\n",
      "  '0.008*\"give\" + 0.008*\"film\"'),\n",
      " (6,\n",
      "  '0.021*\"dweller\" + 0.021*\"wenwu\" + 0.017*\"attack\" + 0.014*\"escape\" + '\n",
      "  '0.014*\"darkness\" + 0.013*\"go\" + 0.012*\"iron\" + 0.011*\"villager\" + '\n",
      "  '0.011*\"gate\" + 0.010*\"xiale\"'),\n",
      " (7,\n",
      "  '0.001*\"movie\" + 0.001*\"go\" + 0.001*\"film\" + 0.001*\"good\" + 0.001*\"fight\" + '\n",
      "  '0.001*\"superhero\" + 0.001*\"great\" + 0.001*\"year\" + 0.001*\"make\" + '\n",
      "  '0.001*\"lot\"'),\n",
      " (8,\n",
      "  '0.001*\"movie\" + 0.001*\"play\" + 0.001*\"good\" + 0.001*\"also\" + 0.001*\"watch\" '\n",
      "  '+ 0.001*\"film\" + 0.001*\"marvel\" + 0.001*\"especially\" + 0.001*\"many\" + '\n",
      "  '0.001*\"give\"'),\n",
      " (9,\n",
      "  '0.026*\"film\" + 0.020*\"great\" + 0.012*\"new\" + 0.011*\"people\" + 0.010*\"good\" '\n",
      "  '+ 0.010*\"fight\" + 0.010*\"superhero\" + 0.010*\"make\" + 0.009*\"character\" + '\n",
      "  '0.009*\"set\"'),\n",
      " (10,\n",
      "  '0.020*\"good\" + 0.020*\"film\" + 0.020*\"really\" + 0.011*\"character\" + '\n",
      "  '0.010*\"action\" + 0.010*\"time\" + 0.010*\"may\" + 0.010*\"well\" + '\n",
      "  '0.010*\"especially\" + 0.010*\"story\"'),\n",
      " (11,\n",
      "  '0.001*\"film\" + 0.001*\"go\" + 0.001*\"good\" + 0.001*\"make\" + 0.001*\"fight\" + '\n",
      "  '0.001*\"year\" + 0.001*\"people\" + 0.001*\"even\" + 0.001*\"dweller\" + '\n",
      "  '0.001*\"character\"'),\n",
      " (12,\n",
      "  '0.020*\"film\" + 0.016*\"action\" + 0.016*\"first\" + 0.012*\"fight\" + 0.012*\"s\" + '\n",
      "  '0.012*\"year\" + 0.008*\"good\" + 0.008*\"visual\" + 0.008*\"fine\" + '\n",
      "  '0.008*\"marvel\"'),\n",
      " (13,\n",
      "  '0.016*\"also\" + 0.014*\"time\" + 0.011*\"good\" + 0.011*\"especially\" + '\n",
      "  '0.011*\"movie\" + 0.011*\"people\" + 0.011*\"make\" + 0.009*\"screen\" + '\n",
      "  '0.009*\"even\" + 0.009*\"big\"'),\n",
      " (14,\n",
      "  '0.011*\"movie\" + 0.011*\"way\" + 0.011*\"may\" + 0.011*\"go\" + 0.011*\"live\" + '\n",
      "  '0.011*\"superhero\" + 0.011*\"danger\" + 0.011*\"friend\" + 0.011*\"believe\" + '\n",
      "  '0.011*\"call\"'),\n",
      " (15,\n",
      "  '0.024*\"film\" + 0.023*\"action\" + 0.022*\"absolutely\" + 0.015*\"story\" + '\n",
      "  '0.015*\"year\" + 0.015*\"character\" + 0.010*\"friendship\" + 0.010*\"anticipate\" '\n",
      "  '+ 0.009*\"load\" + 0.009*\"art\"'),\n",
      " (16,\n",
      "  '0.001*\"movie\" + 0.001*\"have\" + 0.001*\"year\" + 0.001*\"character\" + '\n",
      "  '0.001*\"would\" + 0.001*\"love\" + 0.001*\"give\" + 0.001*\"good\" + 0.001*\"plot\" + '\n",
      "  '0.001*\"could\"'),\n",
      " (17,\n",
      "  '0.001*\"movie\" + 0.001*\"film\" + 0.001*\"scene\" + 0.001*\"would\" + 0.001*\"good\" '\n",
      "  '+ 0.001*\"character\" + 0.001*\"come\" + 0.001*\"have\" + 0.001*\"superhero\" + '\n",
      "  '0.001*\"go\"')]\n",
      "\n",
      "Perplexity:  -7.465608070321993\n",
      "\n",
      "Coherence Score:  0.34410446874433\n"
     ]
    }
   ],
   "source": [
    "# Building the Topic Model with number of topics as \n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=18, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "#View the topics in LDA model\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "#Compute Model Perplexity and Coherence Score\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "#Visualize the topics-keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqW3ldQDfGbx"
   },
   "source": [
    "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR-eaLYkjtR2",
    "outputId": "8a0e32bd-5822-4317-c487-d6db8981b33d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents: 27\n",
      "[(0, '-0.373*\"chi\" + -0.373*\"shang\" + -0.311*\"wenwu\" + -0.269*\"ring\" + -0.226*\"ten\" + -0.160*\"movi\" + -0.146*\"kati\" + -0.122*\"xial\" + -0.118*\"fight\" + -0.106*\"year\"'), (1, '0.510*\"movi\" + 0.223*\"scene\" + 0.191*\"mcu\" + -0.168*\"wenwu\" + -0.158*\"shang\" + -0.158*\"chi\" + 0.150*\"love\" + 0.140*\"watch\" + 0.136*\"charact\" + 0.132*\"like\"'), (2, '0.328*\"film\" + -0.210*\"year\" + -0.189*\"wenwu\" + 0.184*\"marvel\" + 0.182*\"chi\" + 0.182*\"shang\" + -0.164*\"would\" + -0.155*\"ring\" + 0.126*\"great\" + -0.120*\"ten\"'), (3, '-0.258*\"marvel\" + -0.229*\"year\" + -0.206*\"film\" + 0.177*\"scene\" + -0.165*\"charact\" + 0.153*\"movi\" + -0.128*\"live\" + 0.125*\"dark\" + -0.116*\"came\" + -0.115*\"saun\"'), (4, '-0.188*\"show\" + -0.146*\"panther\" + -0.146*\"liu\" + 0.145*\"film\" + -0.142*\"know\" + -0.139*\"act\" + -0.126*\"black\" + -0.122*\"play\" + 0.121*\"scene\" + -0.119*\"lead\"'), (5, '-0.276*\"new\" + -0.168*\"look\" + 0.160*\"marvel\" + -0.149*\"superhero\" + -0.132*\"promis\" + -0.131*\"like\" + -0.130*\"alon\" + -0.123*\"charact\" + -0.123*\"film\" + -0.117*\"actual\"'), (6, '0.212*\"time\" + 0.195*\"peopl\" + 0.165*\"even\" + 0.163*\"also\" + 0.131*\"belong\" + 0.128*\"especi\" + -0.121*\"marvel\" + 0.119*\"lot\" + -0.118*\"movi\" + 0.114*\"last\"')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write your code here\n",
    "\n",
    "#Importing required Library\n",
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import modules\n",
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading Data\n",
    "def load_data(path,file_name):\n",
    "    \n",
    "    the_name_of_documents_list = []\n",
    "    name_of_titles=[]\n",
    "    with open( os.path.join(path, file_name) ,\"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "    print(\"Total Number of Documents:\",len(the_name_of_documents_list))\n",
    "    name_of_titles.append( text[0:min(len(text),100)] )\n",
    "    return the_name_of_documents_list,name_of_titles\n",
    "\n",
    "#preprocessing of Data\n",
    "\n",
    "def preprocess_data(doc_set):\n",
    "    \n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw_data = i.lower()\n",
    "        tokensinfo = tokenizer.tokenize(raw_data)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n",
    "  \n",
    "#Preparing Corpus\n",
    "\n",
    "def prepare_corpus(doc_clean):\n",
    "    \n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionaryinfo = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "#Creating LSA model\n",
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel\n",
    "\n",
    "#\n",
    "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def plot_graph(doc_clean,start, stop, step):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "                                                            stop, start, step)\n",
    "    # Show graph\n",
    "    x = range(start, stop, step)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "    plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "number_of_topics=7\n",
    "words=10\n",
    "document_list,titles=load_data(\"\",\"IMDB_reviews.csv\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "xnCv6vYPpeTM",
    "outputId": "aea4a024-c6e6-43ec-a013-6b808547f094"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start,stop,step=2,15,1\n",
    "plot_graph(clean_text,start,stop,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
    "\n",
    "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lda2vec\n",
      "  Downloading lda2vec-0.16.10.tar.gz (13 kB)\n",
      "Building wheels for collected packages: lda2vec\n",
      "  Building wheel for lda2vec (setup.py): started\n",
      "  Building wheel for lda2vec (setup.py): finished with status 'done'\n",
      "  Created wheel for lda2vec: filename=lda2vec-0.16.10-py3-none-any.whl size=14433 sha256=2ad991b8ff01f97665c4584f5843a446574b08053b95fe5e7122ddcf582780b1\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\74\\d3\\19\\9a711a19d209476de11d2b57df549fad077d2a06e4c391f0b0\n",
      "Successfully built lda2vec\n",
      "Installing collected packages: lda2vec\n",
      "Successfully installed lda2vec-0.16.10\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-c4d578b2560b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python -m pip install -U lda2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lda2vec\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirichlet_likelihood\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdirichlet_likelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_mixture\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0membedding_mixture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLda2vec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLda2vec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embedding\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlda2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlppipe\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnlppipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lda2vec\\dirichlet_likelihood.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdirichlet_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U lda2vec\n",
    "from lda2vec import preprocess, Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "    import seaborn\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'topics.pyldavis.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-98c283bf9eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnpz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'topics.pyldavis.npz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnpz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vocab'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vocab'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# dat['term_frequency'] = dat['term_frequency'] * 1.0 / dat['term_frequency'].sum()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'topics.pyldavis.npz'"
     ]
    }
   ],
   "source": [
    "npz = np.load(open('topics.pyldavis.npz', 'r'))\n",
    "dat = {k: v for (k, v) in npz.iteritems()}\n",
    "dat['vocab'] = dat['vocab'].tolist()\n",
    "# dat['term_frequency'] = dat['term_frequency'] * 1.0 / dat['term_frequency'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 15\n",
    "topic_to_topwords = {}\n",
    "for j, topic_to_word in enumerate(dat['topic_term_dists']):\n",
    "    top = np.argsort(topic_to_word)[::-1][:top_n]\n",
    "    msg = 'Topic %i '  % j\n",
    "    top_words = [dat['vocab'][i].strip()[:35] for i in top]\n",
    "    message += ' '.join(top_words)\n",
    "    print message\n",
    "    topic_to_topwords[j] = top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 0 x11r5 xv window xterm server motif font xlib // sunos\n",
    "Topic 1 jesus son father matthew sin mary g'd disciples christ sins\n",
    "Topic 2 s1 nsa s2 clipper chip administration q escrow private sector serial number encryption technology\n",
    "Topic 3 leafs games playoffs hockey game players pens yankees bike phillies\n",
    "Topic 4 van - 0 pp en 1 njd standings 02 6\n",
    "Topic 5 out_of_vocabulary out_of_vocabulary anonymity hiv homicide adl ripem bullock encryption technology eff\n",
    "Topic 6 hiv magi prof erzurum venus van 2.5 million ankara satellite launched\n",
    "Topic 7 nsa escrow clipper chip encryption government phones warrant vat decrypt wiretap\n",
    "Topic 8 mac controller shipping disk printer mb ethernet enable os/2 port\n",
    "Topic 9 leafs cooper weaver karabagh myers agdam phillies flyers playoffs fired\n",
    "Topic 10 obfuscated = ciphertext jesus gentiles matthew judas { x int\n",
    "Topic 11 jesus ra bobby faith god homosexuality bible sin msg islam\n",
    "Topic 12 jesus sin scripture matthew christ islam god sins prophet faith\n",
    "Topic 13 mac i thanks monitor apple upgrade card connect using windows\n",
    "Topic 14 i quadra monitor my apple duo hard drive mac mouse thanks\n",
    "Topic 15 { shipping } + mac mb os/2 $ 3.5 manuals\n",
    "Topic 16 playoffs morris yankees leafs // pitching players } team wins\n",
    "Topic 17 :> taxes guns flame .. clinton kids jobs hey drugs\n",
    "Topic 18 revolver tires pitching saturn ball trigger car ice team engine\n",
    "Topic 19 stephanopoulos leafs mamma karabagh mr. koresh apartment fired myers sumgait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
    "\n",
    "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.2.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (7.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (0.9.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: pyLDAvis in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.20.1)\n",
      "Requirement already satisfied: numexpr in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: gensim in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.2.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.6.2)\n",
      "Requirement already satisfied: funcy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.24.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: sklearn in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: future in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U spacy\n",
    "!python -m pip install -U pyLDAvis\n",
    "import spacy\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 171, in _merge_into_criterion\n",
      "    crit = self.state.criteria[name]\n",
      "KeyError: 'plotly'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 189, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 178, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 316, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 121, in resolve\n",
      "    self._result = resolver.resolve(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 453, in resolve\n",
      "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 347, in resolve\n",
      "    failure_causes = self._attempt_to_pin_criterion(name, criterion)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 207, in _attempt_to_pin_criterion\n",
      "    criteria = self._get_criteria_to_update(candidate)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 199, in _get_criteria_to_update\n",
      "    name, crit = self._merge_into_criterion(r, parent=candidate)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 173, in _merge_into_criterion\n",
      "    crit = Criterion.from_requirement(self._p, requirement, parent)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 82, in from_requirement\n",
      "    if not cands:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 124, in __bool__\n",
      "    return bool(self._sequence)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in __bool__\n",
      "    return any(self)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 38, in _iter_built\n",
      "    candidate = func()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 167, in _make_candidate_from_link\n",
      "    self._link_candidate_cache[link] = LinkCandidate(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 300, in __init__\n",
      "    super().__init__(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 144, in __init__\n",
      "    self.dist = self._prepare()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 226, in _prepare\n",
      "    dist = self._prepare_distribution()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 311, in _prepare_distribution\n",
      "    return self._factory.preparer.prepare_linked_requirement(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 457, in prepare_linked_requirement\n",
      "    return self._prepare_linked_requirement(req, parallel_builds)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 480, in _prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 230, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 108, in get_http_url\n",
      "    from_path, content_type = download(link, temp_dir.path)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 163, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 159, in iter\n",
      "    for x in it:\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting BERTopic\n",
      "  Downloading bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: pandas>=1.1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from BERTopic) (1.2.4)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
      "Requirement already satisfied: pyyaml<6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from BERTopic) (5.4.1)\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.6.0-py2.py3-none-any.whl (27.7 MB)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bertopic'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-9db425bdd7df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'python -m pip install -U BERTopic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bertopic'"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U BERTopic\n",
    "from bertopic import BERTopic\n",
    "model = BERTopic(verbose=True)\n",
    "df=data[0:500]\n",
    "docs = df\n",
    "topics, probabilities = model.fit_transform(docs)\n",
    "model.get_topic_freq().head(11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVS4WGT8fGby"
   },
   "source": [
    "## (5) (10 points) Compare the results generated by the two topic modeling algorithms, which one is better? You should explain the reasons in details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here (no code needed for this question)\n",
    " \n",
    "As per the coherence score, it can be deduced that LDA algorithm has higher advantage over others with respect to the results obtained.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "In_class_exercise_04 (4).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
